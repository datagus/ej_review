{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88c04cd0-07b5-49bd-83a3-a0b3a4337a4c",
   "metadata": {},
   "source": [
    "# SLR Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43db32b8-f33f-4cb0-8e82-1a40392d8b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pybliometrics.scopus import ScopusSearch, AbstractRetrieval\n",
    "\n",
    "import pybliometrics\n",
    "pybliometrics.scopus.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197cc46c-118b-433b-a110-6b33186633a6",
   "metadata": {},
   "source": [
    "## Search String and first search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2585003-fe84-4ad5-83a4-faa3912af199",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_string = \"\"\"TITLE-ABS(\"environmental justice\" OR \"environmental injustice\")\"\"\"\n",
    "results = ScopusSearch(search_string, verbose = True, download = True)\n",
    "results.get_results_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981157a8-b0b9-4fae-a527-d3beb663e76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5898479e-10bc-4fd9-b437-73d9cdd4f82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_df = pd.DataFrame(results.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09097678-b06f-4272-8bfe-1f5adfb3b6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#putting the eids or identifiers into a list for the Abstract Retrieval Step\n",
    "eids = search_df[\"eid\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4681ebb1-5967-464f-9d76-9353536e70b0",
   "metadata": {},
   "source": [
    "## Importing picke - search_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227cc82e-258c-4af9-839b-86b7f1c821a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#with open('first_search.pkl', 'wb') as file:\n",
    "    #pickle.dump(search_df, file)\n",
    "\n",
    "with open('pickles/first_search.pkl', 'rb') as file:\n",
    "    search_df = pickle.load(file)\n",
    "\n",
    "#creating a copy of the search dataframe\n",
    "filtered_df = search_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4b474f-3af4-4611-8d00-f5d939b66624",
   "metadata": {},
   "source": [
    "## Abstract Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8147f1d-7e56-4c06-92af-772f869fc163",
   "metadata": {},
   "source": [
    "The code cells below are for retrieving the articles information and filter them by type and language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe1abc4-5f86-47ec-b0a4-5929737579a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = []\n",
    "for eid in tqdm(eids):\n",
    "    articles.append(AbstractRetrieval(eid, view='FULL'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41f8364-db8e-4fdb-a38a-0674830783f9",
   "metadata": {},
   "source": [
    "## Import pickle - Abstract retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8361793b-a121-4e45-bc9e-97243f6f8cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#with open('articles_raw_list.pkl', 'wb') as file:\n",
    "#    pickle.dump(articles, file)\n",
    "\n",
    "with open('pickles/articles_raw_list.pkl', 'rb') as file:\n",
    "    articles = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49ecad-7b66-413d-bad9-5ec254df0f58",
   "metadata": {},
   "source": [
    "## Filtering in english articles and articles and reviews, and until 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b71fde-7845-4b43-9dc3-d08d2c8d58ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new columns for language and article type\n",
    "\n",
    "filtered_df[\"language\"] = \"\"\n",
    "filtered_df[\"type\"] = \"\"\n",
    "\n",
    "#articles has the same length in number of rows as filter_df\n",
    "#populating the columns and adding the abstract column\n",
    "for i, article in enumerate(articles):\n",
    "    filtered_df.loc[i,\"language\"] = article.language\n",
    "    filtered_df.loc[i,\"type\"] = article.subtype\n",
    "    filtered_df.loc[i,\"abstract\"] = article.abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d1cd4a-a6bc-42a0-8ed3-0c760757f588",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying filtered selection for articles, review and english language\n",
    "df = filtered_df.loc[((filtered_df[\"type\"] == \"ar\") | (filtered_df[\"type\"] == \"re\")) \n",
    "                    & (filtered_df[\"language\"] == \"eng\"),]\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "#creating a column with only the year\n",
    "df[\"year\"] = df.loc[:,\"coverDate\"].str[:4]\n",
    "\n",
    "#excluding articles from 2025\n",
    "df = df.loc[df[\"year\"] != \"2025\",]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "#dropping articles without abstract\n",
    "df_abs = df.copy()\n",
    "df_abs = df.loc[df[\"abstract\"].isna() == False,]\n",
    "df_abs = df_abs.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daae9065-3063-4a18-b0ab-ebf6ec850bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc45cd67-bf5a-4cda-b4b1-80c256125c0a",
   "metadata": {},
   "source": [
    "## Creating the dataframe for screening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e90e0d1-f128-4459-8cf8-bc8fb92cdbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b62894-c45d-4655-90de-f2adc20f382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_df = df_abs.copy()\n",
    "#selecting only the relevant columns\n",
    "first_df = first_df.loc[:, [\"eid\", \"doi\", \"publicationName\",\n",
    "                          \"author_names\", \"year\", \"title\", \"abstract\"]]\n",
    "\n",
    "#changing the name of some columnts\n",
    "first_df = first_df.rename(columns={\"publicationName\": \"journal\"})\n",
    "\n",
    "#creating a short id for future reference\n",
    "first_df = first_df.reset_index(drop=False, names = \"short_id\")\n",
    "\n",
    "#filling missing values\n",
    "first_df[\"doi\"] = first_df[\"doi\"].fillna(\"no doi\")\n",
    "first_df[\"author_names\"] = first_df[\"author_names\"].fillna(\"no names\")\n",
    "\n",
    "# Create a new column \"weblink\" while preserving the original \"doi\" column\n",
    "\n",
    "# Create DOI Link\n",
    "first_df[\"weblink\"] = (\"https://doi.org/\" + first_df[\"doi\"]).where(first_df[\"doi\"] != \"no doi\", \"no link\")\n",
    "\n",
    "# Create Google Scholar Link\n",
    "first_df[\"scholar_link\"] = (\"https://scholar.google.com/scholar?q=\" + first_df[\"doi\"]).where(first_df[\"doi\"] != \"no doi\", \"no link\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dab8e0f-b4de-4620-98d1-a16660c2d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reordering the columns\n",
    "\n",
    "weblink_col = first_df.pop(\"weblink\")\n",
    "scholar_link_col = first_df.pop(\"scholar_link\")\n",
    "\n",
    "# Find the position of \"doi\"\n",
    "doi_index = first_df.columns.get_loc(\"doi\")\n",
    "\n",
    "# Insert the \"weblink\" column right after \"doi\"\n",
    "first_df.insert(doi_index + 1, \"weblink\", weblink_col)\n",
    "\n",
    "# Insert the \"scholar_link\" column right after \"weblink\"\n",
    "first_df.insert(doi_index + 2, \"scholar_link\", scholar_link_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed3bdba-14a6-4dee-a651-dc1988c73f62",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Shuffling the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a4d15a-082e-4680-ade0-845b80536500",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_df = first_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "shuffled_df[\"included\"] = \"x\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22478964-927d-415c-a696-db3b7e8eeb87",
   "metadata": {},
   "source": [
    "### Loading the pickle with the shuffled dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce086b44-b457-45d5-99dd-cd05060f37be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#with open('pickles/shuffled_df.pkl', 'wb') as file:\n",
    "    #pickle.dump(shuffled_df, file)\n",
    "\n",
    "with open('pickles/shuffled_df.pkl', 'rb') as file:\n",
    "    shuffled_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7280ae-a70d-42cc-8e41-ba4c84f4df67",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_df[\"included\"] = \"-\"\n",
    "shuffled_df[\"article_type\"] = \"-\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de94a24-b0a1-4737-a994-ad0c156809e2",
   "metadata": {},
   "source": [
    "### Importing last edited table\n",
    "\n",
    "This happens when some coders have run a test on the table on google spreadsheets. But to modify the whole table again, without losing th first input from coders, I need to integrate this into the original shuffled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5789bc69-f2bf-4991-9b3c-026d3b396c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheet_id = '1Ai8o0my3obDICNLMUFKcCYv0Q6Od9NWc'  # Replace with your Google Sheet ID\n",
    "sheets_names = ['EF', 'CG', 'HVW', 'GR']  # Replace with your sheet name\n",
    "\n",
    "imported_dfs = []\n",
    "for sheet_name in sheets_names:\n",
    "    csv_url = f'https://docs.google.com/spreadsheets/d/{spreadsheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}'\n",
    "    imported_dfs.append(pd.read_csv(csv_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970eb0f1-97b7-41ce-9150-e17369e340cd",
   "metadata": {},
   "source": [
    "### Getting the data already inserted\n",
    "\n",
    "I am retrieving the data that the coders put already and saving it as lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed51b721-d341-4b51-ae6a-49e7b0c9e0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "EF_coded = list(imported_dfs[0].iloc[:99, -1])\n",
    "CG_coded = list(imported_dfs[1].iloc[:99, -1])\n",
    "HVW_coded = list(imported_dfs[2].iloc[:50, -1])\n",
    "GR_coded = list(imported_dfs[3].iloc[:100, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f7e403-57e4-42eb-a3f7-0efd7e484b84",
   "metadata": {},
   "source": [
    "### Distribution of the abstract screening table\n",
    "\n",
    "Here is the distribution of articles per coders for the abstract screening process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c232eed-16b3-4e6d-8461-9f84e699852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = {\n",
    "    \"Elli\": [\"EF\", 800],\n",
    "    \"Charlotte\": [\"CG\", 800],\n",
    "    \"Henrik\": [\"HVW\", 300],\n",
    "    \"Gustavo\": [\"GR\", 1000],\n",
    "    \"Wanja\": [\"WT\", 100],\n",
    "    \"Joanna\": [\"JK\", 100],\n",
    "    \"Victor\": [\"VC\", 100],\n",
    "    \"Max\": [\"MFK\", 100],\n",
    "    \"Kristina\": [\"KBB\", 400],\n",
    "    \"Clara\": [\"CUW\", 200],\n",
    "    \"Antonia\": [\"AU\", 100],\n",
    "    \"Dagmar\": [\"DBM\", 400],\n",
    "    \"Madawi\": [\"MN\", 100],\n",
    "    \"Emre\": [\"EO\", 100],\n",
    "    \"Luana\": [\"LK\", 100],\n",
    "    \"Jaqueline\": [\"JL\", 100],\n",
    "    \"Polyana\": [\"PL\", 150],\n",
    "    \"Extra3\": [\"EX3\", 100],\n",
    "    \"Extra4\": [\"EX4\", 100],\n",
    "    \"Extra5\": [\"EX5\", 100],\n",
    "    \"Extra6\": [\"EX6\", 100],\n",
    "    \"Extra7\": [\"EX7\", 100],\n",
    "    \"Extra8\": [\"EX8\", 100],\n",
    "    \"Extra9\": [\"EX9\", 126]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2552ef1-ef99-43a6-a6f2-6dfa3b76f88c",
   "metadata": {},
   "source": [
    "### Creating different dataframes for each coder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecdd93e-ec57-442b-a6d7-3dec60659373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this piece of code is to divide automatically the dataframe according to the distribution\n",
    "dfs = []\n",
    "start_index= 0\n",
    "end_index = 0\n",
    "for key, value in distribution.items():\n",
    "    num = value[1] # contains the number of abstracts to be screened\n",
    "    end_index = end_index + num \n",
    "    table = shuffled_df.iloc[start_index:end_index,:]\n",
    "    table2 = table.copy()\n",
    "    dfs.append(table2) \n",
    "    distribution[key].append(table)\n",
    "    start_index = start_index + num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59e5912-11ab-4c7d-a1a6-27ba8f40acc4",
   "metadata": {},
   "source": [
    "### Updating the respective dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb58322-059b-4ad7-96e9-db96b9a0a98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution[\"Elli\"][2].iloc[:99,-2] = EF_coded\n",
    "distribution[\"Charlotte\"][2].iloc[:99,-2] = CG_coded\n",
    "distribution[\"Henrik\"][2].iloc[:50,-2] = HVW_coded\n",
    "distribution[\"Gustavo\"][2].iloc[:100,-2] = GR_coded\n",
    "\n",
    "#changing these columns to string\n",
    "\n",
    "distribution[\"Elli\"][2].iloc[:,-2] = distribution[\"Elli\"][2].iloc[:,-2].astype(str)\n",
    "distribution[\"Charlotte\"][2].iloc[:,-2] = distribution[\"Charlotte\"][2].iloc[:,-2].astype(str)\n",
    "distribution[\"Henrik\"][2].iloc[:,-2] = distribution[\"Henrik\"][2].iloc[:,-2].astype(str)\n",
    "distribution[\"Gustavo\"][2].iloc[:,-2] = distribution[\"Gustavo\"][2].iloc[:,-2].astype(str)\n",
    "\n",
    "distribution[\"Elli\"][2].loc[distribution[\"Elli\"][2][\"included\"] == \"1.0\", [\"included\"]] = \"yes\"\n",
    "distribution[\"Elli\"][2].loc[distribution[\"Elli\"][2][\"included\"] == \"0.0\", [\"included\"]] = \"no\"\n",
    "distribution[\"Charlotte\"][2].loc[distribution[\"Charlotte\"][2][\"included\"] == \"1.0\", [\"included\"]] = \"yes\"\n",
    "distribution[\"Charlotte\"][2].loc[distribution[\"Charlotte\"][2][\"included\"] == \"0.0\", [\"included\"]] = \"no\"\n",
    "distribution[\"Henrik\"][2].loc[distribution[\"Henrik\"][2][\"included\"] == \"1\", [\"included\"]] = \"yes\"\n",
    "distribution[\"Henrik\"][2].loc[distribution[\"Henrik\"][2][\"included\"] == \"0\", [\"included\"]] = \"no\"\n",
    "distribution[\"Gustavo\"][2].loc[distribution[\"Gustavo\"][2][\"included\"] == \"1.0\", [\"included\"]] = \"yes\"\n",
    "distribution[\"Gustavo\"][2].loc[distribution[\"Gustavo\"][2][\"included\"] == \"0.0\", [\"included\"]] = \"no\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff0d8b6-27c5-4288-ad71-37576f39f564",
   "metadata": {},
   "source": [
    "### Putting the test input in the respective coders' dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e16ac1-1eef-4bba-adc2-807a0d907eac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "distribution[\"Charlotte\"][2].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5530511d-95cc-452e-8c0c-284bc81cc64d",
   "metadata": {},
   "source": [
    "# Creating Excel File with highlighting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df46988f-ead5-4b44-a9d5-561a5d9bd1c2",
   "metadata": {},
   "source": [
    "## Function for coloring strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1aa64f-6204-4bd2-b70c-7de9a2aba24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# to search the regular expression in the string\n",
    "pattern = re.compile(r'(environmental injustice|environmental justice)',re.IGNORECASE)\n",
    "\n",
    "def highlight_keywords(worksheet, row, col, text):\n",
    "    \"\"\"\n",
    "    Splits 'text' around each regex match (pattern)\n",
    "    and writes partial substrings using XlsxWriter's write_rich_string().\n",
    "    Matches for 'environmental injustice' or 'environmental justice' are in red.\n",
    "    Everything else is in black.\n",
    "    \"\"\"\n",
    "    # Find all matches + the segments in between\n",
    "    matches = list(pattern.finditer(text))\n",
    "    \n",
    "    # If no matches, write the entire text in black\n",
    "    if not matches:\n",
    "        worksheet.write(row, col, text, black_format)\n",
    "        return\n",
    "    \n",
    "    # Build a list of alternating segments with their formats\n",
    "    rich_segments = []\n",
    "    last_end = 0\n",
    "    \n",
    "    for match in matches:\n",
    "        start, end = match.span()\n",
    "        \n",
    "        # Add text before the match, in black\n",
    "        if start > last_end:\n",
    "            segment_before = text[last_end:start]\n",
    "            rich_segments.append(black_format)\n",
    "            rich_segments.append(segment_before)\n",
    "        \n",
    "        # Add the matched substring, in red\n",
    "        match_str = text[start:end]\n",
    "        rich_segments.append(red_format)\n",
    "        rich_segments.append(match_str)\n",
    "        \n",
    "        last_end = end\n",
    "    \n",
    "    # Add any remaining text after the last match, in black\n",
    "    if last_end < len(text):\n",
    "        segment_after = text[last_end:]\n",
    "        rich_segments.append(black_format)\n",
    "        rich_segments.append(segment_after)\n",
    "    \n",
    "    # Check if there are fewer than two format/text pairs\n",
    "    if len(rich_segments) <= 2:\n",
    "        # If there's only one fragment, fall back to normal write()\n",
    "        # Default to black or red format depending on the first match\n",
    "        worksheet.write(row, col, text, red_format if matches else black_format)\n",
    "    else:\n",
    "        # Use write_rich_string for multiple fragments\n",
    "        worksheet.write_rich_string(row, col, *rich_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fec5ac-4343-46eb-b977-b8e52fa92f07",
   "metadata": {},
   "source": [
    "## Creating Excel writer using xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe368553-5f51-47d5-a55a-546de91672a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "\n",
    "output_file = \"pilot2.xlsx\"\n",
    "writer = pd.ExcelWriter(output_file, engine='xlsxwriter')\n",
    "\n",
    "# Add a blank workbook & worksheet\n",
    "workbook  = writer.book\n",
    "\n",
    "# Define formats for highlighting\n",
    "red_format = workbook.add_format({'font_color': '#DA70D6', 'bold': True, 'valign': 'vcenter'})\n",
    "black_format = workbook.add_format({'font_color': 'black', 'text_wrap': True, 'valign': 'vcenter'})\n",
    "header_format = workbook.add_format({'font_color': 'white', 'bold': True, 'align': 'center', 'bg_color': 'green'})\n",
    "align_format = workbook.add_format({'text_wrap': True, 'valign': 'vcenter'}) \n",
    "\n",
    "for key,val in distribution.items():\n",
    "    person_id = val[0]  #the person id\n",
    "    df = val[-1]      #the respective dataframe\n",
    "    nrows = val[1] + 1 # the number of rows fo the dataframe\n",
    "    worksheet = workbook.add_worksheet(person_id)\n",
    "\n",
    "\n",
    "\n",
    "    dropdown_options = ['yes', 'no', '-']\n",
    "    category_col_letter = 'K'  # Assuming \"category\" is in column K\n",
    "    worksheet.data_validation(f'{category_col_letter}2:{category_col_letter+str(nrows)}', {\n",
    "    'validate': 'list',\n",
    "    'source': dropdown_options,\n",
    "    'input_message': 'Select value',\n",
    "    'error_message': 'Invalid selection! Please choose from the dropdown.'\n",
    "    })\n",
    "\n",
    "    dropdown_options2 = ['empirical', 'conceptual', 'review', 'unknown', '-']\n",
    "    category_col_letter2 = 'L'  # Assuming \"category\" is in column K\n",
    "    worksheet.data_validation(f'{category_col_letter2}2:{category_col_letter2+str(nrows)}', {\n",
    "    'validate': 'list',\n",
    "    'source': dropdown_options2,\n",
    "    'input_message': 'Select value',\n",
    "    'error_message': 'Invalid selection! Please choose from the dropdown.'\n",
    "    })\n",
    "    \n",
    "    # writing header row\n",
    "    for col_idx, col_name in enumerate(df.columns):\n",
    "        worksheet.write(0, col_idx, col_name, header_format)\n",
    "\n",
    "    # Write data rows\n",
    "    for row_idx in range(len(df)):\n",
    "        for col_idx, value in enumerate(df.iloc[row_idx]):\n",
    "            col_name = df.columns[col_idx]\n",
    "            \n",
    "            if col_name in [\"title\", \"abstract\"] and isinstance(value, str):\n",
    "                # Apply partial substring highlighting\n",
    "                highlight_keywords(worksheet, row_idx + 1, col_idx, value)\n",
    "\n",
    "                char_per_line = 50  # Adjust based on actual column width\n",
    "                num_lines = -(-len(value) // char_per_line)  # Ceiling division for line count\n",
    "                row_height = max(7 * num_lines, 7)\n",
    "                worksheet.set_row(row_idx + 1, row_height)  # Set row height\n",
    "\n",
    "            elif col_name in [\"included\", \"article_type\"]:\n",
    "            # Write category values normally (dropdown will apply to this column)\n",
    "                worksheet.write(row_idx + 1, col_idx, value, black_format)\n",
    "\n",
    "            else:\n",
    "                # Write other columns normally\n",
    "                worksheet.write(row_idx + 1, col_idx, value, black_format)\n",
    "\n",
    "    #adjusting size of columns\n",
    "    worksheet.set_column('E:E', 15)\n",
    "    worksheet.set_column('G:G', 15)  \n",
    "    worksheet.set_column('I:I', 50, align_format)  \n",
    "    worksheet.set_column('J:J', 100, align_format)\n",
    "    worksheet.set_column('K:K', 10) \n",
    "    worksheet.set_column('L:L', 17) \n",
    "\n",
    "# Close (this actually writes the file)\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3d24c6-4983-42eb-9a40-75422605e2ea",
   "metadata": {},
   "source": [
    "# Uploading to google sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aee16f-5de8-4447-bd10-ee52f5c68017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "import gspread\n",
    "\n",
    "# Define the required scopes\n",
    "scopes = [\n",
    "    \"https://www.googleapis.com/auth/drive\",\n",
    "    \"https://www.googleapis.com/auth/spreadsheets\"\n",
    "]\n",
    "\n",
    "# Authenticate using the credentials file\n",
    "flow = InstalledAppFlow.from_client_secrets_file('google_credentials.json', scopes=scopes)\n",
    "credentials = flow.run_local_server(port=0)\n",
    "\n",
    "# Initialize Google Drive and Google Sheets API clients\n",
    "drive_service = build('drive', 'v3', credentials=credentials)\n",
    "gc = gspread.authorize(credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a52c9f-2b21-4b2e-9dc8-ab20844500e0",
   "metadata": {},
   "source": [
    "### Creating Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a8f1e3-190c-4422-b737-66d4cd1deb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder in Google Drive\n",
    "folder_metadata = {\n",
    "    'name': 'Environmental Justice - SLR',  # Folder name\n",
    "    'mimeType': 'application/vnd.google-apps.folder'\n",
    "}\n",
    "\n",
    "folder = drive_service.files().create(body=folder_metadata, fields='id').execute()\n",
    "folder_id = folder.get('id')\n",
    "print(f\"Folder created successfully! Folder ID: {folder_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50685148-1dda-40e7-b997-8c1ae558b2a0",
   "metadata": {},
   "source": [
    "### Creating Spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8ba804-42ef-4b0b-bc1c-7c85d1c15045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Google Spreadsheet in the folder\n",
    "spreadsheet_metadata = {\n",
    "    'name': 'abstract_screening',  # Spreadsheet name\n",
    "    'mimeType': 'application/vnd.google-apps.spreadsheet',\n",
    "    'parents': [folder_id]  # Place in the created folder\n",
    "}\n",
    "\n",
    "spreadsheet = drive_service.files().create(body=spreadsheet_metadata, fields='id,webViewLink').execute()\n",
    "spreadsheet_id = spreadsheet.get('id')\n",
    "spreadsheet_url = spreadsheet.get('webViewLink')\n",
    "\n",
    "print(f\"Spreadsheet created successfully! URL: {spreadsheet_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62db20be-d07f-4d0d-9475-9c9303bf5b4c",
   "metadata": {},
   "source": [
    "### Updating spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fb876d-3c4b-4b0e-a012-408b8289ecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {\"Ellie\": elli_df, \n",
    "              \"Charlotte\": charlotte_df, \n",
    "              \"Gustavo\": gustavo_df}\n",
    "\n",
    "spreadsheet = gc.open_by_key(spreadsheet_id)\n",
    "\n",
    "for sheet_name, df in dataframes.items():\n",
    "    # Create a new worksheet or open if it exists\n",
    "    try:\n",
    "        worksheet = spreadsheet.add_worksheet(title=sheet_name, rows=str(df.shape[0]+10), cols=str(df.shape[1]+10))\n",
    "    except gspread.exceptions.APIError:\n",
    "        worksheet = spreadsheet.worksheet(sheet_name)\n",
    "    \n",
    "    # Write data to the worksheet\n",
    "    data = [df.columns.values.tolist()] + df.values.tolist()  # Convert DataFrame to list of lists\n",
    "    worksheet.update(data)\n",
    "\n",
    "print(\"DataFrames uploaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a518ae02-2fb2-49b5-8ae4-b4d924bc5261",
   "metadata": {},
   "source": [
    "### Deleting sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95534bb9-6e90-404b-884d-5b3d700bce18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the spreadsheet by its ID\n",
    "spreadsheet = gc.open_by_key(spreadsheet_id)\n",
    "\n",
    "# Find and delete \"Sheet1\"\n",
    "try:\n",
    "    worksheet = spreadsheet.worksheet(\"Sheet1\")\n",
    "    spreadsheet.del_worksheet(worksheet)\n",
    "    print(\"Default 'Sheet1' deleted successfully!\")\n",
    "except gspread.exceptions.WorksheetNotFound:\n",
    "    print(\"'Sheet1' does not exist or has already been deleted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b18bbf-fb8f-4bb7-9f80-6b437219a073",
   "metadata": {},
   "source": [
    "### Changing permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6863d58-112d-4118-85d6-1e6a0893e62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Share the file with \"Anyone with the link\" and set to \"Editor\"\n",
    "drive_service.permissions().create(\n",
    "    fileId=spreadsheet_id,  # The ID of your spreadsheet\n",
    "    body={\n",
    "        'type': 'anyone',  # Share with anyone\n",
    "        'role': 'writer'   # Grant editing permissions\n",
    "    },\n",
    "    fields='id'\n",
    ").execute()\n",
    "\n",
    "print(\"The spreadsheet is now editable by anyone with the link!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e559cab-4b89-4ed0-941f-031421d80027",
   "metadata": {},
   "source": [
    "### Creating Log Spreadsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48c2b82-9458-4a36-9b63-146589c971c9",
   "metadata": {},
   "source": [
    "#### Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e5a220-fc2b-4c11-b66e-e195c6ef19db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Define the scopes for Google Drive and Sheets\n",
    "scopes = [\n",
    "    \"https://www.googleapis.com/auth/drive\",\n",
    "    \"https://www.googleapis.com/auth/spreadsheets\"\n",
    "]\n",
    "\n",
    "# Authenticate using OAuth 2.0\n",
    "flow = InstalledAppFlow.from_client_secrets_file(\n",
    "    'google_credentials.json',  # Path to your OAuth credentials file\n",
    "    scopes=scopes\n",
    ")\n",
    "credentials = flow.run_local_server(port=0)\n",
    "\n",
    "# Initialize the Drive API\n",
    "drive_service = build('drive', 'v3', credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b8561e-9271-4d45-b29a-d867790807c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_id = \"1jQgEHv8m7ZtXeBoiQtp2e61i9FB-7Dpc\"\n",
    "\n",
    "# Create a new Google Sheets file in the folder\n",
    "spreadsheet_metadata = {\n",
    "    'name': 'Change Log',  # Name of the new spreadsheet\n",
    "    'mimeType': 'application/vnd.google-apps.spreadsheet',\n",
    "    'parents': [folder_id]  # Place it inside the folder\n",
    "}\n",
    "\n",
    "spreadsheet = drive_service.files().create(\n",
    "    body=spreadsheet_metadata,\n",
    "    fields='id,webViewLink'\n",
    ").execute()\n",
    "\n",
    "# Retrieve the spreadsheet ID and URL\n",
    "spreadsheet_id = spreadsheet.get('id')\n",
    "spreadsheet_url = spreadsheet.get('webViewLink')\n",
    "\n",
    "print(f\"Change Log spreadsheet created successfully: {spreadsheet_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cea16a-5f3d-48d9-b272-59f07d7d9a80",
   "metadata": {},
   "source": [
    "## Uploading excel workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2d3d56-f802-474c-9dc1-3ee2d761ba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gspread_dataframe import set_with_dataframe\n",
    "\n",
    "# Path to the Excel file\n",
    "excel_file = \"pilot.xlsx\"\n",
    "\n",
    "# Load the Excel file with pandas\n",
    "excel_data = pd.read_excel(excel_file, sheet_name=None)  # Load all sheets as a dictionary\n",
    "\n",
    "folder_id = \"1jQgEHv8m7ZtXeBoiQtp2e61i9FB-7Dpc\"\n",
    "\n",
    "# Create a Google Spreadsheet in the folder\n",
    "spreadsheet_metadata = {\n",
    "    'name': 'abstract_screening',  # Spreadsheet name\n",
    "    'mimeType': 'application/vnd.google-apps.spreadsheet',\n",
    "    'parents': [folder_id]  # Place in the created folder\n",
    "}\n",
    "\n",
    "spreadsheet = drive_service.files().create(body=spreadsheet_metadata, fields='id,webViewLink').execute()\n",
    "spreadsheet_id = spreadsheet.get('id')\n",
    "spreadsheet_url = spreadsheet.get('webViewLink')\n",
    "\n",
    "print(f\"Spreadsheet created successfully! URL: {spreadsheet_url}\")\n",
    "\n",
    "spreadsheet = gc.open_by_key(spreadsheet_id)\n",
    "\n",
    "# Iterate over sheets in the Excel file and upload each sheet\n",
    "for sheet_name, sheet_data in excel_data.items():\n",
    "    # Add a new worksheet for each sheet\n",
    "    worksheet = spreadsheet.add_worksheet(title=sheet_name, rows=str(sheet_data.shape[0] + 1), cols=str(sheet_data.shape[1]))\n",
    "    \n",
    "    # Upload the data from the Excel sheet to the Google Sheet\n",
    "    set_with_dataframe(worksheet, sheet_data)\n",
    "\n",
    "# Remove the default worksheet (if empty)\n",
    "default_sheet = spreadsheet.sheet1\n",
    "spreadsheet.del_worksheet(default_sheet)\n",
    "\n",
    "print(f\"Excel workbook uploaded successfully to Google Sheets: {spreadsheet.url}\")\n",
    "\n",
    "# Share the file with \"Anyone with the link\" and set to \"Editor\"\n",
    "drive_service.permissions().create(\n",
    "    fileId=spreadsheet_id,  # The ID of your spreadsheet\n",
    "    body={\n",
    "        'type': 'anyone',  # Share with anyone\n",
    "        'role': 'writer'   # Grant editing permissions\n",
    "    },\n",
    "    fields='id'\n",
    ").execute()\n",
    "\n",
    "print(\"The spreadsheet is now editable by anyone with the link!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "general"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
